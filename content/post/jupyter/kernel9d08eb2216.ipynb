{"cells":[{"metadata":{},"cell_type":"markdown","source":"***The purpose is of this notebook to submit my project of Data Mining to practice classifier and feature engineering.****\n\n***The reference for this code is taken from the following:**\n1. https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial\n2. https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n3. https://www.kaggle.com/holfyuen/basic-nlp-on-disaster-tweets\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Import Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import libraries required\n\nimport seaborn as sns\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Load data files\ntrain_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample_submission_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nprint (train_data.shape)\nprint (test_data.shape)\nprint (sample_submission_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Refine Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove duplicate values from train_data\n\ntrain_data = train_data.drop_duplicates().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check blank values in train_data\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check blank values in test_data\ntest_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are very less blank values for \"keyword\" and more blank values for \"location\" in train as well as test data\n\nSo, we don't need to replace blank \"keyword\" and for blank \"location\" we will check later if it is very much affecting our target."},{"metadata":{},"cell_type":"markdown","source":"## 3. Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check number of unique \"keywords\"\nprint (\"Train data unique keywords\", train_data.keyword.nunique())\nprint (\"Test data unique keywords\", test_data.keyword.nunique())\n\n#We can see the number of unique keywords are same for both datasets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find out the Top 20 keywords for disaster and non-disaster"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common \"keywords\"\n\nplt.figure(figsize=(9,6))\n\nsns.countplot(y=train_data.keyword, order = train_data.keyword.value_counts().iloc[:25].index)\nplt.title('Top 20 keywords')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_d = train_data[train_data.target==1].keyword.value_counts().head(20)\nkey_nd = train_data[train_data.target==0].keyword.value_counts().head(20)\n\nplt.figure(figsize=(13,5))\n\nplt.subplot(121)\nsns.barplot(key_d, key_d.index, color='green')\nplt.title('Top 20 keywords for disaster tweets')\n\nplt.subplot(122)\nsns.barplot(key_nd, key_nd.index, color='red')\nplt.title('Top 20 keywords for non-disaster tweets')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_key_d = train_data.groupby('keyword').mean()['target'].sort_values(ascending=False).head(20)\ntop_key_nd = train_data.groupby('keyword').mean()['target'].sort_values().head(20)\n\nplt.figure(figsize=(13,5))\n\nplt.subplot(121)\nsns.barplot(top_key_d, top_key_d.index, color='blue')\nplt.title('Keywords with highest percentage of disaster tweets')\n\nplt.subplot(122)\nsns.barplot(top_key_nd, top_key_nd.index, color='orange')\nplt.title('Keywords with lowest percentage of disaster tweets')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We didn't find any common values for top-20 disaster adn non-disaster keywords between train_data and test_data"},{"metadata":{},"cell_type":"markdown","source":"## 4. Locations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check number of unique \"locations\"\nprint (\"Train data unique locations\", train_data.location.nunique())\nprint (\"Test data unique locations\", test_data.location.nunique())\n\n# We can see the number of unique locations are not same for both datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 20 Locations\n\nplt.figure(figsize=(9,6))\nsns.countplot(y=train_data.location, order = train_data.location.value_counts().iloc[:20].index)\n\nplt.title('Top 20 locations')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are same locations with different names like \"USA\" and \"United States\", we can merge them to get cleaner data.\nAlso we need to check percentage of disaster tweets for these locations."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_loc = train_data.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\ntop_only = train_data[train_data.location.isin(top_loc)]\n\ntop_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l.index, y=top_l)\nplt.axhline(np.mean(train_data.target))\nplt.xticks(rotation=80)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Clean missing and duplicate values\n\nAs we discussed earlier, we need to clean data for further processing as it is going to have a big effect on our resultant target values.\n\nAlso, we need to merge the locations with same meaning.\nfor eg. \"USA\" and \"United States\", \"New York City\" and \"NYC\", etc"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-fill missing values\nfor col in ['keyword','location']:\n    train_data[col] = train_data[col].fillna('None')\n    test_data[col] = test_data[col].fillna('None')\n\n\n# Merge locations with same meaning\ndef clean_loc(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_loc:\n        return x\n    else: return 'Others'\n    \ntrain_data['location_clean'] = train_data['location'].apply(lambda x: clean_loc(str(x)))\ntest_data['location_clean'] = test_data['location'].apply(lambda x: clean_loc(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-check cleaned value for train_data\nprint(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-check cleaned value for test_data\nprint(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot cleaned location data on graph\n\ntop_l2 = train_data.groupby('location_clean').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l2.index, y=top_l2)\nplt.axhline(np.mean(train_data.target))\nplt.xticks(rotation=80)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Clean text in the columns\n\nHere we clean up the text column by:\n\nMaking a 'clean' text column, removing links and unnecessary white spaces\nCreating separate columns containing lists of hashtags, mentions, and links"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning keywords from train_data\n\nimport re\n\nfor col in train_data['keyword']:\n    test_str = col\n    def clean_text(text):\n        text = re.sub(r'%20+', '_', text) # Replace %20 with Underscore\n        \n        return text\n\n    print(\"Original text: \" + test_str)\n    print(\"Cleaned text: \" + clean_text(test_str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning keywords from test_data\n\nimport re\n\nfor col in test_data['keyword']:\n    test_str = col\n    def clean_text(text):\n        text = re.sub(r'%20+', '_', text) # Replace %20 with Underscore\n        \n        return text\n\n    # print(\"Original text: \" + test_str)\n    # print(\"Cleaned text: \" + clean_text(test_str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning Tweet Text from train_data\n\nimport re\n\nfor col in train_data['text']:\n    test_str = col\n    def clean_text(text):\n        text = re.sub(r'%20+', ' ', text) # Replace %20 with Space\n        text = re.sub(r'&amp;+', '&', text) # Replace &amp with \"&\" symbol\n        text = re.sub(r'https?://\\S+', '', text) # Remove link\n        text = re.sub(r'\\n',' ', text) # Remove line breaks\n        text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n        return text\n\n    print(\"Original tweet: \" + test_str)\n    print(\"Cleaned tweet: \" + clean_text(test_str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning Tweet Text from test_data\n\nimport re\n\nfor col in test_data['text']:\n    test_str = col\n    def clean_text(text):\n        text = re.sub(r'%20+', ' ', text) # Replace %20 with Space\n        text = re.sub(r'&amp;+', '&', text) # Replace &amp with \"&\" symbol\n        text = re.sub(r'https?://\\S+', '', text) # Remove link\n        text = re.sub(r'\\n',' ', text) # Remove line breaks\n        text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n        return text\n\n    # print(\"Original tweet: \" + test_str)\n    # print(\"Cleaned tweet: \" + clean_text(test_str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorizing data based on hashtags, mentions, and links for both train_data & test_data\n\ndef find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n\ndef find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n\ndef find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", tweet)]) or 'no'\n\ndef process_text(df):\n    \n    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    \n    return df\n    \ntrain_data = process_text(train_data)\ntest_data = process_text(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Perform more refinement \n## (based on Stopwords, Punctuation, etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for all stopwords, punctuations, etc. from the cleaned data\n\nfrom wordcloud import STOPWORDS\n\ndef create_stat(df):\n    # Tweet length\n    df['text_len'] = df['text_clean'].apply(len)\n    # Word count\n    df['word_count'] = df[\"text_clean\"].apply(lambda x: len(str(x).split()))\n    # Stopword count\n    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n    # Punctuation count\n    df['punctuation_count'] = df['text_clean'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    # Count of hashtags (#)\n    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(str(x).split()))\n    # Count of mentions (@)\n    df['mention_count'] = df['mentions'].apply(lambda x: len(str(x).split()))\n    # Count of links\n    df['link_count'] = df['links'].apply(lambda x: len(str(x).split()))\n    # Count of uppercase letters\n    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n    # Ratio of uppercase letters\n    df['caps_ratio'] = df['caps_count'] / df['text_len']\n    return df\n\ntrain_data = create_stat(train_data)\ntest_data = create_stat(test_data)\n\nprint(train_data.shape, test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort and find out the correlation of all \"counts\" with \"target\"\n\ntrain_data.corr()['target'].drop('target').sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Use Refined Data for Target Encoding\n\nApplying Feature Engineering:\n- Apply target encoding to keyword and location (cleaned)\n- Count Vectorize cleaned text, links, hashtags and mentions columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\n\n# Target encoding\nfeatures = ['keyword', 'location_clean']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train_data[features],train_data['target'])\n\ntrain_data = train_data.join(encoder.transform(train_data[features]).add_suffix('_target'))\ntest_data = test_data.join(encoder.transform(test_data[features]).add_suffix('_target'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vectorize the cleaned text\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# CountVectorizer\n\n# Links\nvec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?://\\S+') # Only include those >=5 occurrences\nlink_vec = vec_links.fit_transform(train_data['links'])\nlink_vec_test = vec_links.transform(test_data['links'])\nX_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\nX_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())\n\n# Mentions\nvec_men = CountVectorizer(min_df = 5)\nmen_vec = vec_men.fit_transform(train_data['mentions'])\nmen_vec_test = vec_men.transform(test_data['mentions'])\nX_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\nX_test_men = pd.DataFrame(men_vec_test.toarray(), columns=vec_men.get_feature_names())\n\n# Hashtags\nvec_hash = CountVectorizer(min_df = 5)\nhash_vec = vec_hash.fit_transform(train_data['hashtags'])\nhash_vec_test = vec_hash.transform(test_data['hashtags'])\nX_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\nX_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())\n\nprint (X_train_link.shape, X_train_men.shape, X_train_hash.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate percentage of disaster given by links\n_ = (X_train_link.transpose().dot(train_data['target']) / X_train_link.sum(axis=0)).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate percentage of disaster given by mentions\n_ = (X_train_men.transpose().dot(train_data['target']) / X_train_men.sum(axis=0)).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check hashtags with which 100% of Tweets are disasters\nhash_rank = (X_train_hash.transpose().dot(train_data['target']) / X_train_hash.sum(axis=0)).sort_values(ascending=False)\nprint('Hashtags with which 100% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==1].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==1])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check hashtags with which 100% of Tweets are disasters\nprint('Hashtags with which 0% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==0].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tf-idf for text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n# Only include >=10 occurrences\n# Have unigrams and bigrams\ntext_vec = vec_text.fit_transform(train_data['text_clean'])\ntext_vec_test = vec_text.transform(test_data['text_clean'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\nprint (X_train_text.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Joining the dataframes together\n\ntrain_data = train_data.join(X_train_link, rsuffix='_link')\ntrain_data = train_data.join(X_train_men, rsuffix='_mention')\ntrain_data = train_data.join(X_train_hash, rsuffix='_hashtag')\ntrain_data = train_data.join(X_train_text, rsuffix='_text')\ntest_data = test_data.join(X_test_link, rsuffix='_link')\ntest_data = test_data.join(X_test_men, rsuffix='_mention')\ntest_data = test_data.join(X_test_hash, rsuffix='_hashtag')\ntest_data = test_data.join(X_test_text, rsuffix='_text')\nprint (train_data.shape, test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Logistic Regression Model\n\nWe try the simplest model with logistic regression, based on all features we created above. Before we fit a model, we first transform the features into the same scale with minimum 0 and maximum 1. \n\nWe do this in the form of pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\nfeatures_to_drop = ['id', 'keyword','location','text','location_clean','text_clean', 'hashtags', 'mentions','links']\nscaler = MinMaxScaler()\n\nX_train = train_data.drop(columns = features_to_drop + ['target'])\nX_test = test_data.drop(columns = features_to_drop)\ny_train = train_data.target\n\nlr = LogisticRegression(solver='liblinear', random_state=777) # Other solvers have failure to converge problem\n\npipeline = Pipeline([('scale',scaler), ('lr', lr),])\n\npipeline.fit(X_train, y_train)\ny_test = pipeline.predict(X_test)\n\nsubmit = sample_submission_data.copy()\nsubmit.target = y_test\nsubmit.to_csv('submit_lr.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the Training Accuracy\nprint ('Training accuracy: %.4f' % pipeline.score(X_train, y_train))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}